{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc2a41df",
   "metadata": {},
   "source": [
    "# LangChain LLM Examples\n",
    "LangChain is a framework that provides, among others, a set of Python APIs to ease the use of Large Language Models.  \n",
    "This notebook shows a few basic examples to get started, see https://python.langchain.com/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46a56a1",
   "metadata": {},
   "source": [
    "# OpenAI example\n",
    "OpenAI provides a state of the art LLM, via an API. The model inference is perfomed on OpenAI platform. You need to register to get an API key. There is a limited amount of compute that can be done for free, see also pricing: https://openai.com/pricing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a41beffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "\n",
    "from getpass import getpass\n",
    "OPENAI_API_KEY = getpass()\n",
    "\n",
    "# Get an OPenAI API Key from https://platform.openai.com/\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "# This will use OpenAI models, the default model currently is text-davinci-003\n",
    "llm=OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "425d96cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseModel.json of OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, openai_api_base=None, openai_organization=None, openai_proxy=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all')>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d405691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "\n",
      "1. Measure out the desired amount of rice. For every cup of rice, you will need two cups of water.\n",
      "\n",
      "2. Rinse the rice in a fine mesh strainer until the water runs clear.\n",
      "\n",
      "3. Place the rinsed rice in a pot and add the measured water.\n",
      "\n",
      "4. Bring the water to a boil over high heat.\n",
      "\n",
      "5. Once boiling, reduce the heat to low and cover the pot with a lid.\n",
      "\n",
      "6. Simmer the rice for 15-20 minutes, or until all the water has been absorbed.\n",
      "\n",
      "7. Remove the pot from the heat and let the rice sit, covered, for 10 minutes.\n",
      "\n",
      "8. Fluff the rice with a fork and serve.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"How to cook rice?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c53573b",
   "metadata": {},
   "source": [
    "# Open access models, download and run on local GPU\n",
    "There is a variety of models available for open access. However they currently do not perfom as well as the commercial ones and require machine with powerful GPUs to run the more advanced model.  \n",
    "Here we just provide some basic example of small models that can fit on a T4.  \n",
    "Note download the models from the web and uploading the weights into the GPU can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a31cf896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap a model downloaded from HuggingFace with the LangChain API\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "pipe = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device=0)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "969a8be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a small saucepan or microwave safe bowl and add enough water to cover the rice by 1-2 cm. Bring the pot or bowl to a boil, reduce heat and let cook for 15-20 minutes until the water is mostly absorbed.\n",
      "\n",
      "Step 3. Fluff with a fork and enjoy warm\n",
      "\n",
      "It's ready when you pour it out of the pot or bowl and it's been cooled on a dish.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"How to cook rice?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88069efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
